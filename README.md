# Lighthouse
<img src="https://github.com/LorenzoPozzi97/lighthouse/assets/83987444/283cb75c-c033-4b9d-8dcd-668b4054ad18" width="400" height="400">

## Benchmarking inference of quantized LLM models
Performance of a model can remarkably vary depending many factors: hardware, model architecture, kernel, etc. The lighthouse intention is sheding light on LLMs inference performance. Navigating the seas of models can be strenuous, that is why lighthouse provides functionalities to quickly understand the potention of your model across different configurations.

## Setup
To use the fuctionalities of the lighthouse on your machine just clone the repo with:
```
git clone https://github.com/LorenzoPozzi97/lighthouse.git
```
Call the script setup.sh that will create a virtual environemnt with all the necessary packages and activate the venv:
```
bash setup.sh
source .lighthouse/bin/activate
```
### Supported platforms
- [x] Linux
- [ ] Windows
- [ ] Mac OS
      
## What you can do
Here is a list of the metrics that is currenty possible to track with the lighthouse:
- Model load time
- Prompt processing time
- Token generation time

### Supported quantization formats
- [x] GGUF
- [ ] GPTQ
- [ ] AWQ

## Usage
The lighthouse works in three steps:
- decide the parameters of your experiments
- store the results in your personal database, namely the bulb ðŸ’¡
- interrogate the bulb ðŸ’¡ to create interactive graphs

Here is a typical interaction with the library to test a modle quantized in gguf format:
```
python ./lighthouse/benchmark_gguf.py --model-path solar-10.7b-instruct-v1.0.Q4_K_M.gguf
```
Each configuration in the experiment will be appended to you ðŸ’¡. 
To create an interactive parallel coordinated graphs use:
```
python ./lighthouse/parallel_coordinates.py
```
or for a 2D graph (with the name autogenerated for the test):
```
python ./lighthouse/bidimensional_graphs.py --run-anchor straightforward_turkey_trot
```

## Contributions
Any contribution is very well appreciated! This project is still in its embryonal stage but it could save a lot of time to many people.
